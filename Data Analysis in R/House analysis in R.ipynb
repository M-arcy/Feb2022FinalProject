{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6bf47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'dplyr' was built under R version 4.1.2\"\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error: package or namespace load failed for 'dplyr' in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace 'rlang' 0.4.11 is already loaded, but >= 1.0.0 is required\n",
     "output_type": "error",
     "traceback": [
      "Error: package or namespace load failed for 'dplyr' in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace 'rlang' 0.4.11 is already loaded, but >= 1.0.0 is required\nTraceback:\n",
      "1. library(\"dplyr\")",
      "2. tryCatch({\n .     attr(package, \"LibPath\") <- which.lib.loc\n .     ns <- loadNamespace(package, lib.loc)\n .     env <- attachNamespace(ns, pos = pos, deps, exclude, include.only)\n . }, error = function(e) {\n .     P <- if (!is.null(cc <- conditionCall(e))) \n .         paste(\" in\", deparse(cc)[1L])\n .     else \"\"\n .     msg <- gettextf(\"package or namespace load failed for %s%s:\\n %s\", \n .         sQuote(package), P, conditionMessage(e))\n .     if (logical.return && !quietly) \n .         message(paste(\"Error:\", msg), domain = NA)\n .     else stop(msg, call. = FALSE, domain = NA)\n . })",
      "3. tryCatchList(expr, classes, parentenv, handlers)",
      "4. tryCatchOne(expr, names, parentenv, handlers[[1L]])",
      "5. value[[3L]](cond)",
      "6. stop(msg, call. = FALSE, domain = NA)"
     ]
    }
   ],
   "source": [
    "library(\"dplyr\")\n",
    "library(\"corrplot\")\n",
    "library(\"corpcor\")\n",
    "library(\"GPArotation\")\n",
    "library(\"psych\")\n",
    "library(\"IDPmisc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89bead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "house <- read.csv('C:/Users/Marcy/Documents/Entity Coursework/Final Project/house.csv')\n",
    "x <- read.csv(\"C:/Users/Marcy/Documents/Entity Coursework/Final Project/x.csv\")\n",
    "y <- read.csv(\"C:/Users/Marcy/Documents/Entity Coursework/Final Project/y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail(house)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff05b8",
   "metadata": {},
   "source": [
    "#### Create a model that uses SalePrice as the response variable and the other columns as potential predictor variables. Start by creating a function called FitAll that creates a linear model of all predictor variables, which have been recoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c54c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FitAll = lm(SalePrice ~ ., data = house)\n",
    "summary(FitAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4fb85",
   "metadata": {},
   "source": [
    "#### Features significantly tied to house prices \n",
    "#Some of the individual predictors are significant, and they're what you might expect: Overall quality, overall conditin, year built, roofing material, first and second floor square footage, basement full bath, functional, garage cars. Zoning, lot area, Screen porch and sale condition are important to a lesser degree. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e150b97",
   "metadata": {},
   "source": [
    "#### Hybrid Stepwise - Forward and Backward Selection\n",
    "#Starting with no predictors, using the mean for HousePrice only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf75b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitstart = lm(SalePrice~1, data = house)\n",
    "step(fitstart, direction=\"both\", scope=formula(FitAll))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f72b",
   "metadata": {},
   "source": [
    "#### The AIC with no predictors at all is 32946.74. Adding anything at all will bring the AIC down, but the second iteration shows that the best way to improve the model is to add OverallQual of the house as a predictor. \n",
    "Iterating through each of the independent variables, finally at the end, we achieve an AIC of 29376.69. The predictor variables that most improve the model are SalePriceRange + OverallQual + GrLivArea + YearBuilt + \n",
    "    OverallCond + GarageCars + FireplaceQu + RoofMatl + BsmtFullBath + \n",
    "    BldgType + KitchenQual + LotArea + Functional + GarageType + \n",
    "    HeatingQC + ScreenPorch + SaleCondition + MSZoning + GarageYrBlt + \n",
    "    KitchenAbvGr + Street + WoodDeckSF + Fireplaces + BsmtQual + \n",
    "    X1stFlrSF + BsmtFinType1 + GarageFinish + FullBath + LotFrontage + \n",
    "    BsmtHalfBath\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf71cb",
   "metadata": {},
   "source": [
    "#### This does bring to mind more questions: do the importance of these features to homebuyers in this area hold true for homebuyers in other parts of the country?  Is a pool important to homebuyers across the (diving) board, so to speak? \n",
    "Removing the particular neighborhoods listed in this dataset may change the outcome of the regression model. Is it even necessary to remove the neighborhoods for my purposes. But to generalize this across the nation, specific locations or neighborhoods would not be useful. Also, is a heating system as important to homebuyers in Arizona as it would be to those settling down in Michigan? This dataset is located in Iowa, so heating systems are important. In Florida, basements are rare. Would they still boost a home's sale price? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0414d",
   "metadata": {},
   "source": [
    "#### Exploratory Factor Analysis\n",
    "There are three assumptions: sample size, absence of multicollinearity and some relationship between the independent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463de56",
   "metadata": {},
   "source": [
    "#### I'm not going to subset the data, so I'll go straight to a cor matrix to test for the assumption of the absence of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = subset(house, select = -c(SalePrice))\n",
    "houseCor <- cor(subset)\n",
    "View(round(houseCor, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144871c2",
   "metadata": {},
   "source": [
    "#### look for any correlations that are higher than .9. This would indicate really high multicollinearity, and if there's an item that has a correlation of .9, I  will most likely want to remove that item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531150ca",
   "metadata": {},
   "source": [
    "#### I don't see any higher than .55  \n",
    "to doublecheck my findings from the correlation matrix, I'll run a Bartlett's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cortest.bartlett(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae232d2",
   "metadata": {},
   "source": [
    "#### check determinants, which is another measure of how the variables relate to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1febf8a3",
   "metadata": {},
   "source": [
    "#### Initial pass to determine approximate number of factors. Because I didn't subset, I'm going to try 20 factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcZ <- principal(subset, nfactors = 20, rotate = \"none\")\n",
    "pcZ\n",
    "#pcZ.loadings.df <- data.frame(pcZ$loading)\n",
    "#write.csv(pcZ.loadings.df, \"ss_loadings.csv\")\n",
    "#write.csv(data.frame(pcZ$loading > 1.0), \"loadings_above_one.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c048b",
   "metadata": {},
   "source": [
    "#### R has tried to break down the data into 20 factors. On the first pass is the SS loadings on the bottom, which contains the eigenvalues. \n",
    "The larger the eigenvalue, the more likely the factor is important. Typically there is a cutoff of 1, so any factors with an eigenvalue > 1 should be examined. \n",
    "In this table, all the PC values are greater than 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b95d8",
   "metadata": {},
   "source": [
    "#### Examine the scree plot for a more visual return of eigenvalues generated by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18707e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(pcZ$values, type=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05d961",
   "metadata": {},
   "source": [
    "#### There's a sheer dropoff after 10-12 factors, down to 50, where the trendline continues its descent. I'll go with top 20 factors and test that assumption, to see if the model fit improves with 12 factors since that's about where the rest flow together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcZ2 <- principal (subset, nfactors = 20, rotate = \"none\")\n",
    "pcZ2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8dac9",
   "metadata": {},
   "source": [
    "#### examining residuals to determine model fit\n",
    " The basic idea behind this test is that the model fits your data very well if there is very little difference between the correlation matrix and the loadings generated through your model. The difference between them is known as the residual. A general rule of thumb is that you have good model fit if the percentage of large residuals (over .05) is less than 50%. In order to make all this easier, you will go through a series of steps. The first line creates your residuals, using the factor.residuals() function. The argument it takes are your correlation matrix and the loadings from your most recent factor analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals <- factor.residuals(subset, pcZ2$loadings)\n",
    "residuals <- as.matrix(residuals[upper.tri(residuals)])\n",
    "largeResid <- abs(residuals) > .05\n",
    "sum(largeResid)\n",
    "sum(largeResid/nrow(residuals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64649b69",
   "metadata": {},
   "source": [
    "#### 89% of the residuals are large.\n",
    "This is over 50%, so having 12 factors is a pretty good model fit for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6f6f1",
   "metadata": {},
   "source": [
    "#### Oblique Rotation\n",
    "Oblimin is the most commonly used type of oblique rotation and I'll keep 12 fators because an examination of the residuals showed that was a good fit for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcZ3 <- principal(subset, nfactors=20, rotate = \"oblimin\")\n",
    "pcZ3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d671579",
   "metadata": {},
   "source": [
    "#### exporting to take a look at the pattern matrix, highlight the values that load highly (anything above .3-.4 loads on that factor)\n",
    "#pcZ3 <- write.csv(pcZ3, \"C:/Users/Marcy/Documents/Entity Coursework/Final Project/pcZ3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905128cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print.psych(pcZ3, cut = .3, sort = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e8ab5",
   "metadata": {},
   "source": [
    "### import unemployment numbers to illustrate Iowa's spike in unemployment during the stay at home order in 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment <- read.csv (\"C:/Users/Marcy/Documents/Entity Coursework/Final Project/Unemployment Insurance Claims.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae64540",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(unemployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"ggplot2\")\n",
    "ggplot(subset(unemployment, Geography != 'Iowa'), aes(x=Dates, y='Unemployment Insurance Claims')) +\n",
    "    geom_line(aes(color= 'gray', group = Geography)) +\n",
    "    geom_line(data = subset(unemployment, Geography == 'Iowa'), size=3, color = 'blue') +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3fe50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
